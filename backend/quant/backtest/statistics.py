import numpy as np
import pandas as pd
import scipy.stats as stats

def estimated_sharpe_ratio_variance(returns: pd.Series) -> float:
    """
    Calculate the estimated variance of the Sharpe Ratio.
    Ref: Lo (2002), "The Statistics of Sharpe Ratios"
    
    Var(SR) approx = (1/T) * (1 + 0.5 * SR^2 - skew * SR + (kurtosis-3)/4 * SR^2)
    Actually, simpler approx often used:
    V = (1/T) * (1 - skew*SR + (kurt-1)/4 * SR^2)
    """
    if len(returns) < 2:
        return 0.0
        
    sr = sharpe_ratio(returns)
    skew = stats.skew(returns)
    kurt = stats.kurtosis(returns, fisher=False) # Fisher=False means Pearson's definition (normal=3)
    
    # Formula from Bailey & Lopez de Prado (2012)
    # V[SR] = (1 / (T-1)) * (1 - skew * SR + ((kurt - 1) / 4) * SR**2)
    
    T = len(returns)
    var_sr = (1 / (T - 1)) * (1 - skew * sr + ((kurt - 1) / 4) * sr**2)
    return var_sr

def sharpe_ratio(returns: pd.Series, risk_free: float = 0.0, periods: int = 252) -> float:
    """Annualized Sharpe Ratio"""
    if returns.std() == 0:
        return 0.0
    return (returns.mean() - risk_free) / returns.std() * np.sqrt(periods)

def probabilistic_sharpe_ratio(returns: pd.Series, benchmark_sr: float = 0.0) -> float:
    """
    Probabilistic Sharpe Ratio (PSR).
    Probability that the true SR is greater than the benchmark SR.
    """
    sr = sharpe_ratio(returns)
    # Variance of daily SR
    var_sr_daily = estimated_sharpe_ratio_variance(returns)
    
    # Annualize variance: Var(SR_ann) = 252 * Var(SR_daily)
    # Assuming returns are daily
    var_sr_ann = var_sr_daily * 252
    
    if var_sr_ann <= 0:
        return 0.0
        
    # Z-score
    z = (sr - benchmark_sr) / np.sqrt(var_sr_ann)
    
    # CDF
    return stats.norm.cdf(z)

def deflated_sharpe_ratio(returns: pd.Series, trials: int = 1, variance_sr: float = None) -> float:
    """
    Deflated Sharpe Ratio (DSR).
    Adjusts PSR for the number of trials (Multiple Testing Bias).
    
    :param returns: Returns series of the strategy selected.
    :param trials: Number of independent strategies tested (N).
    :param variance_sr: Variance of the Sharpe Ratios of the trials (optional).
                        If None, assumes variance=1 (standard normal assumption for SR distribution).
    """
    sr = sharpe_ratio(returns)
    
    # Calculate Expected Maximum Sharpe Ratio under Null Hypothesis
    # E[max SR] approx sqrt(2 * ln(trials)) * std_sr
    # Assuming SRs are normally distributed with mean 0 and variance var_sr
    
    # If we don't know the variance of the trial SRs, we can estimate it from the returns themselves
    # or assume a standard distribution.
    # Lopez de Prado suggests estimating variance of the SRs across the trials.
    # If we only have the "best" strategy returns, we assume the underlying distribution variance.
    
    # Simplified DSR:
    # Benchmark SR becomes the Expected Max SR.
    
    # Euler-Mascheroni constant
    emc = 0.5772156649
    
    # Expected Max Z-score for N independent Gaussian trials
    expected_max_z = (1 - emc) * stats.norm.ppf(1 - 1/trials) + emc * stats.norm.ppf(1 - 1/(trials * np.e))
    
    # If trials=1, expected_max_z is 0 (benchmark)
    if trials <= 1:
        return probabilistic_sharpe_ratio(returns, benchmark_sr=0.0)
        
    # We treat the "Benchmark SR" as this Expected Max SR
    # But we need to scale it by the variance of the SR distribution.
    # Since we are testing a single strategy after the fact, we often use the variance of *this* strategy's SR estimator.
    
    # Actually, the formula is:
    # DSR = PSR(SR, benchmark=ExpectedMaxSR)
    
    # We need the variance of the SRs *across the trials*.
    # If not provided, we can't perfectly calculate it. 
    # A common assumption is that the trial SRs have variance similar to the strategy's SR variance?
    # No, that's the estimation error.
    
    # Let's use the simplified assumption that we are comparing against a "Zero Skill" null hypothesis
    # where the max SR generated by luck follows the distribution defined by 'trials'.
    
    # Benchmark SR = Expected Max SR from N trials of noise
    # Since noise has SR=0, variance=1/T (approx), we can scale.
    # But usually DSR is defined on annualized SR.
    
    # Let's implement the version that takes the 'expected_max_sr' directly if possible,
    # or calculates it assuming standard normal distribution of annual SRs (sigma=1 is too high).
    # Usually sigma of annual SRs for random strategies is around 0.5?
    
    # For this implementation, we will use the calculated SR variance of the strategy itself
    # as a proxy for the variance of the distribution of SRs (conservative).
    
    var_sr_daily = estimated_sharpe_ratio_variance(returns)
    var_sr_ann = var_sr_daily * 252
    
    if var_sr_ann <= 0:
        return 0.0
        
    # Expected Max SR (Benchmark)
    # We assume the trials were drawn from a distribution with mean 0 and variance = var_sr_estimator
    # This is a strong assumption but allows calculation without full trial history.
    benchmark_sr = expected_max_z * np.sqrt(var_sr_ann)
    
    # Calculate PSR against this benchmark
    dsr = probabilistic_sharpe_ratio(returns, benchmark_sr=benchmark_sr)
    
    return dsr
